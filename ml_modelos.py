# -*- coding: utf-8 -*-
"""ML_modelos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xo9GTzsS7fNpA6gtN9bhb_vFtGBme46H
"""

# Importamos librerías
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, MinMaxScaler,  OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from joblib import dump

# Cargamos los datos
Data = pd.read_csv("/content/data_adults.csv")
Data.head()

# Vista rápida de los datos
print(Data.info())

# Eliminamos columnas irrelevantes
Data_cop = Data.drop("fnlwgt", axis=1)
Data_cop = Data_cop.drop("education-num", axis=1)

X = Data_cop.drop("income", axis=1)
y = Data_cop['income'].isin(['>50K.','>50K'])

# Definimos las variables categóricas y numéricas
categorical_features = X.select_dtypes(include=['object']).columns
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns

# Creamos pipelines de preprocesamiento
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Usamos ColumnTransformer para combinar ambas transformaciones
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Definimos el pipeline completo
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Dividimos los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Creamos un nuevo pipeline que incluye el preprocesamiento y el modelo
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                 ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])

# Entrenamos el pipeline completo en los datos de entrenamiento
model_pipeline.fit(X_train, y_train)

# Hacemos predicciones en el conjunto de prueba
y_pred = model_pipeline.predict(X_test)

# Evaluamos el rendimiento del modelo
print(classification_report(y_test, y_pred))

model_pipeline

modelo_entrenado=model_pipeline['classifier']
preprocessor_entrenado=model_pipeline['preprocessor']

nombres=preprocessor_entrenado.get_feature_names_out()
importancia=modelo_entrenado.feature_importances_

df_features=pd.DataFrame()
df_features['nombres']=nombres
df_features['importancia']=importancia
df_features=df_features.sort_values('importancia',ascending=False)
df_features.head(20)

plt.barh(df_features.head(20)['nombres'],df_features.head(20)['importancia'])

# Guardamos el pipeline completo (preprocesamiento + modelo entrenado)
dump(model_pipeline, 'model_pipeline_rf.joblib')

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV


# Cargamos los datos
Data = pd.read_csv("/content/data_adults.csv")

# Eliminamos columnas irrelevantes
Data_cop = Data.drop("fnlwgt", axis=1)
Data_cop = Data_cop.drop("education-num", axis=1)

X = Data_cop.drop("income", axis=1)
y = Data_cop['income'].isin(['>50K.','>50K'])


# Definimos las variables categóricas y numéricas
categorical_features = X.select_dtypes(include=['object']).columns
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns

# Creamos pipelines de preprocesamiento
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Usamos ColumnTransformer para combinar ambas transformaciones
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

param_grid={
    'n_estimators':[50,100,150,200],
    'max_depth':[2,5,10],
    'min_samples_split':[2,5,10]
}
model_rf=RandomForestClassifier(random_state=2024)

#### Instanciamos la busqueda del mejor modelo

model_busqueda=GridSearchCV(estimator=model_rf,
                            param_grid=param_grid,
                            cv=3,
                            verbose=4,
                            scoring='f1',
                            n_jobs=-1)

model_pipeline_train= Pipeline(steps=[('preprocessor', preprocessor),
                                 ('busqueda', model_busqueda)])

# Entrenamos el pipeline completo en los datos de entrenamiento
model_pipeline_train.fit(X_train, y_train)

busqueda_resultado=model_pipeline_train['busqueda']
busqueda_resultado.best_estimator_.get_params()

y_pred=model_pipeline_train.predict(X_test)

print(classification_report(y_test,y_pred))

y_pred=model_pipeline.predict(X_test)
print(classification_report(y_test,y_pred))

y_pred_train=model_pipeline_train.predict(X_train)
print(classification_report(y_train,y_pred_train))

y_pred_train=model_pipeline.predict(X_train)
print(classification_report(y_train,y_pred_train))

model_pipeline['classifier'].get_params()

# Importar las librerías necesarias
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Cargar el dataset
data = pd.read_csv('/content/data_adults.csv')  # Asegúrate de que la ruta sea correcta

# Identificar características y objetivo
X = data.drop('income', axis=1)  # Reemplaza 'target' con el nombre de tu columna objetivo
y = data['income']  # Reemplaza 'target' con el nombre de tu columna objetivo

# Identificar columnas numéricas y categóricas
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

# Preprocesamiento: escalado para características numéricas y One-Hot Encoding denso para categóricas
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ]
)

# Definir los pipelines para cada modelo
pipelines = {
    'SVC': Pipeline([
        ('preprocessor', preprocessor),
        ('svc', SVC())
    ]),
    'Naive Bayes': Pipeline([
        ('preprocessor', preprocessor),
        ('nb', GaussianNB())
    ]),
    'K-Nearest Neighbors': Pipeline([
        ('preprocessor', preprocessor),
        ('knn', KNeighborsClassifier())
    ]),
    'Gradient Boosting': Pipeline([
        ('preprocessor', preprocessor),
        ('gboost', GradientBoostingClassifier())
    ])
}

# Dividir en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenar y evaluar cada pipeline
results = {}
for model_name, pipeline in pipelines.items():
    # Entrenar el modelo
    pipeline.fit(X_train, y_train)

    # Predecir
    y_pred = pipeline.predict(X_test)

    # Calcular la precisión y el informe de clasificación
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    # Guardar los resultados
    results[model_name] = {
        'Accuracy': accuracy,
        'Classification Report': report
    }
    print(f"Modelo: {model_name}")
    print(f"Precisión: {accuracy}")
    print(f"Reporte de Clasificación:\n{report}")
    print("="*50)

# Comparación de modelos
best_model = max(results, key=lambda x: results[x]['Accuracy'])
print(f"El mejor modelo es: {best_model} con una precisión de {results[best_model]['Accuracy']}")